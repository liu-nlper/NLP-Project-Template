#!/usr/bin/env python
# -*- encoding: utf-8 -*-
"""
    符号化:
        1. 英文的符号化
        2. 中文的分词
        3. ...
"""


class Tokenizer(object):

    def __init__(self):
        """
        初始化符号化模块
        """
        print('test import Tokenizer...')

    def tokenize(self, text):
        """
        对text进行符号化

        Args:
            text: str, 待符号化的句子

        Returns:
            tokens: list of str, 符号化后的token list
        """
        pass
